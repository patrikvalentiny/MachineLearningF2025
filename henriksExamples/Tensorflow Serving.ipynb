{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP Regressor trained with the California housing dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common imports\n",
    "import sys\n",
    "import os\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import the SSL module and configure certificate verification to be optional to avoid SSLCertVerificationError.\n",
    "import ssl\n",
    "ssl.SSLContext.verify_mode = ssl.VerifyMode.CERT_OPTIONAL\n",
    "\n",
    "# Ignore useless warnings (see SciPy issue #5998)\n",
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\", message=\"^internal gelsd\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load, split and scale the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(housing.data, housing.target, random_state=42)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_valid = scaler.transform(X_valid)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.068558169089147"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Computing the mean median_house_value.\n",
    "# We can see that the target value have been scaled down by a factor of 100000\n",
    "# compared to the Housing dataset used in chapter 2. We should keep this in mind,\n",
    "# if we want to compare the RMSE of this model with the RMSEs of the other models\n",
    "# that we trained and tested using the dataset from chapter 2.\n",
    "housing.target.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20640, 8)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing.data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build, compile, train and evaluate a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a model.\n",
    "# We don't need to specify an input layer, since we don't need to convert the input array.\n",
    "# For regression problems, we don't use an activation function in the output layer.\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    # hidden layer\n",
    "    keras.layers.Dense(30, activation=\"relu\", input_shape=X_train.shape[1:]),\n",
    "    # output layer\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "# Compile the model.\n",
    "# For regression problems, we use the \"mean_squared_error\" as loss function.\n",
    "model.compile(loss=\"mean_squared_error\", optimizer=\"sgd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "134/363 [==========>...................] - ETA: 0s - loss: 1.2067 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-24 20:26:19.158698: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "363/363 [==============================] - 0s 823us/step - loss: 0.7838 - val_loss: 3.1488\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 0s 434us/step - loss: 0.5304 - val_loss: 5.1705\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 0s 419us/step - loss: 0.5254 - val_loss: 9.4534\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 0s 427us/step - loss: 0.4746 - val_loss: 0.6727\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 0s 434us/step - loss: 0.4186 - val_loss: 0.3869\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 0s 412us/step - loss: 0.4045 - val_loss: 0.3803\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 0s 421us/step - loss: 0.4027 - val_loss: 0.3678\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 0s 420us/step - loss: 0.3938 - val_loss: 0.3735\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 0s 423us/step - loss: 0.3887 - val_loss: 0.3780\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 0s 428us/step - loss: 0.3863 - val_loss: 0.3752\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 0s 474us/step - loss: 0.3823 - val_loss: 0.3795\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 0s 436us/step - loss: 0.3807 - val_loss: 0.3891\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 0s 425us/step - loss: 0.3761 - val_loss: 0.3907\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 0s 433us/step - loss: 0.3729 - val_loss: 0.3831\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 0s 425us/step - loss: 0.3743 - val_loss: 0.3826\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 0s 413us/step - loss: 0.3705 - val_loss: 0.3726\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 0s 414us/step - loss: 0.3683 - val_loss: 0.3722\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 0s 424us/step - loss: 0.3665 - val_loss: 0.3680\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 0s 417us/step - loss: 0.3786 - val_loss: 0.3646\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 0s 429us/step - loss: 0.3668 - val_loss: 0.3707\n"
     ]
    }
   ],
   "source": [
    "# Train the model.\n",
    "history = model.fit(X_train, y_train, epochs=20, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Show the learning curves.         \n",
    "pd.DataFrame(history.history).plot(figsize=(8, 5))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0, 1)\n",
    "plt.show()         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162/162 [==============================] - 0s 318us/step - loss: 0.3598\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5998126005534559"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the model (outputs the MSE).\n",
    "loss = model.evaluate(X_test, y_test)\n",
    "\n",
    "# Compute and display RMSE\n",
    "np.sqrt(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Observation:</b><br/>\n",
    "If we multipy the RMSE (=loss) with 100000, we can compare with values obtained by the best models that I trained and tested on the dataset used in chapter 2.\n",
    "\n",
    "Gradient Boosted Forest: 46798 (best model that I trained and tested on the dataset used in chapter 2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 35ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.70086485],\n",
       "       [1.7042394 ],\n",
       "       [4.078005  ]], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make predictions for the first 3 instances in the test set.\n",
    "X_new = X_test[:3]\n",
    "y_pred = model.predict(X_new)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.477  , 0.458  , 5.00001])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compare with the corresponding values target values\n",
    "y_test[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Observation:</b><br/>\n",
    "The second prediction is very bad given the RMSE. The other two predictions are okay."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning\n",
    "We can tune hyperparameters by using Scikit-Learn's <b>GridSearchCV</b> or <b>RandomizedSearchCV</b>. The first will train the network with every combination of the specified hyperparameters, while the latter will randomly pick a number of combinations. For this example, we will use RandomizedSearchCV to avoid getting too many combinations.\n",
    "\n",
    "We need to wrap the compiled Keras model in an object that mimic a regular Scikit-Learn regressor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a compiled Keras model and wrap it in a Scikit-Learn KerasRegressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function that will build and compile a Keras model.\n",
    "def build_model(n_hidden=1, n_neurons=30, learning_rate=3e-3, input_shape=[8]):\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.InputLayer(input_shape=input_shape))\n",
    "    for layer in range(n_hidden):\n",
    "        model.add(keras.layers.Dense(n_neurons, activation=\"relu\"))\n",
    "    model.add(keras.layers.Dense(1))\n",
    "    optimizer = keras.optimizers.SGD(lr=learning_rate)\n",
    "    model.compile(loss=\"mse\", optimizer=optimizer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vk/5kv1kws928bgc801_2lf08c40000gn/T/ipykernel_20924/805132444.py:2: DeprecationWarning: KerasRegressor is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  keras_reg = keras.wrappers.scikit_learn.KerasRegressor(build_model)\n"
     ]
    }
   ],
   "source": [
    "# Wrap the Keras model in a Scikit-Learn KerasRegressor.\n",
    "keras_reg = keras.wrappers.scikit_learn.KerasRegressor(build_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use RandomizedSearchCV to tune hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hk/miniconda3/lib/python3.9/site-packages/keras/optimizers/optimizer_v2/gradient_descent.py:111: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "/Users/hk/miniconda3/lib/python3.9/site-packages/keras/optimizers/optimizer_v2/gradient_descent.py:111: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "/Users/hk/miniconda3/lib/python3.9/site-packages/keras/optimizers/optimizer_v2/gradient_descent.py:111: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "/Users/hk/miniconda3/lib/python3.9/site-packages/keras/optimizers/optimizer_v2/gradient_descent.py:111: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "/Users/hk/miniconda3/lib/python3.9/site-packages/keras/optimizers/optimizer_v2/gradient_descent.py:111: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "/Users/hk/miniconda3/lib/python3.9/site-packages/keras/optimizers/optimizer_v2/gradient_descent.py:111: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "/Users/hk/miniconda3/lib/python3.9/site-packages/keras/optimizers/optimizer_v2/gradient_descent.py:111: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "/Users/hk/miniconda3/lib/python3.9/site-packages/keras/optimizers/optimizer_v2/gradient_descent.py:111: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "/Users/hk/miniconda3/lib/python3.9/site-packages/keras/optimizers/optimizer_v2/gradient_descent.py:111: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "/Users/hk/miniconda3/lib/python3.9/site-packages/keras/optimizers/optimizer_v2/gradient_descent.py:111: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "2022-11-24 20:26:27.059802: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2022-11-24 20:26:27.060723: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2022-11-24 20:26:27.063958: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2022-11-24 20:26:27.066204: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2022-11-24 20:26:27.068695: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2022-11-24 20:26:27.073131: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2022-11-24 20:26:27.079378: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2022-11-24 20:26:27.080052: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2022-11-24 20:26:27.081113: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2022-11-24 20:26:27.101503: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "Epoch 1/100\n",
      "Epoch 1/100\n",
      "Epoch 1/100\n",
      "Epoch 1/100\n",
      "Epoch 1/100\n",
      "Epoch 1/100\n",
      "Epoch 1/100\n",
      "Epoch 1/100\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5449 - val_loss: 0.7162\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
      "  1/242 [..............................] - ETA: 0s - loss: nanEpoch 2/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5533 - val_loss: 4.0439\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6420 - val_loss: 1.6753\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 3/100\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4477 - val_loss: 0.9203\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3856 - val_loss: 0.4048\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4359 - val_loss: 1911.8323\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "135/242 [===============>..............] - ETA: 0s - loss: nanEpoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "  1/242 [..............................] - ETA: 0s - loss: nanEpoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3643 - val_loss: 0.3854\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3884 - val_loss: 0.3718\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.5647 - val_loss: 2.0712\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5247 - val_loss: 0.4056\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3516 - val_loss: 0.3843\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3559 - val_loss: 2.1115\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 761us/step - loss: 0.3726 - val_loss: 0.8398\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3460 - val_loss: 0.3589\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4076 - val_loss: 0.3678\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 7/100\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3409 - val_loss: 0.3324\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3896 - val_loss: 0.3816\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3422 - val_loss: 0.4183\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3790 - val_loss: 0.3505\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 964us/step - loss: 0.3311 - val_loss: 0.3571\n",
      " 81/242 [=========>....................] - ETA: 0s - loss: nanEpoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      " 12/242 [>.............................] - ETA: 1s - loss: nanEpoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3310 - val_loss: 0.3591\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "  1/242 [..............................] - ETA: 0s - loss: nanEpoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3260 - val_loss: 0.3160\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3753 - val_loss: 0.3556\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3299 - val_loss: 0.3872\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 861us/step - loss: nan - val_loss: nan\n",
      "Epoch 10/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3166 - val_loss: 0.5307\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3226 - val_loss: 1.1814\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3669 - val_loss: 0.3613\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "121/121 [==============================] - 0s 915us/step - loss: nan\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3165 - val_loss: 0.3908\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3570 - val_loss: 0.3553\n",
      "Epoch 11/100\n",
      "121/121 [==============================] - 0s 946us/step - loss: nan\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3161 - val_loss: 1.5040\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "121/121 [==============================] - 0s 860us/step - loss: nan\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "139/242 [================>.............] - ETA: 0s - loss: 0.3542[CV] END ........learning_rate=0.5, n_hidden=1, n_neurons=56; total time=   3.5s\n",
      "Epoch 1/100\n",
      "121/121 [==============================] - 0s 1ms/step - loss: nan\n",
      "121/121 [==============================] - 0s 624us/step - loss: nan\n",
      "121/121 [==============================] - 0s 935us/step - loss: nan\n",
      "[CV] END ........learning_rate=0.5, n_hidden=1, n_neurons=60; total time=   3.6s\n",
      "Epoch 1/100\n",
      "121/121 [==============================] - 0s 968us/step - loss: nan\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3515 - val_loss: 0.3510\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 887us/step - loss: 0.3101 - val_loss: 0.7921\n",
      "Epoch 12/100\n",
      "[CV] END ........learning_rate=0.5, n_hidden=1, n_neurons=56; total time=   3.6s\n",
      "Epoch 1/100\n",
      "  1/242 [..............................] - ETA: 41s - loss: 9.0990[CV] END ........learning_rate=0.5, n_hidden=1, n_neurons=56; total time=   3.7s\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3101 - val_loss: 0.9066\n",
      "Epoch 12/100\n",
      " 54/242 [=====>........................] - ETA: 0s - loss: nan    [CV] END ........learning_rate=0.5, n_hidden=3, n_neurons=44; total time=   3.7s\n",
      "Epoch 1/100\n",
      "112/242 [============>.................] - ETA: 0s - loss: 0.3543[CV] END ........learning_rate=0.5, n_hidden=3, n_neurons=44; total time=   3.7s\n",
      "Epoch 1/100\n",
      "[CV] END ........learning_rate=0.5, n_hidden=3, n_neurons=44; total time=   3.7s\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3474 - val_loss: 0.3264\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3121 - val_loss: 0.3349\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3100 - val_loss: 0.3576\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "  1/242 [..............................] - ETA: 0s - loss: nanEpoch 2/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3416 - val_loss: 0.3283\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3055 - val_loss: 0.2904\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3019 - val_loss: 0.8981\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6589 - val_loss: 14.5775\n",
      "Epoch 3/100\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5621 - val_loss: 0.3913\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3365 - val_loss: 0.3385\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3015 - val_loss: 0.9677\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3028 - val_loss: 0.3602\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "170/242 [====================>.........] - ETA: 0s - loss: nanEpoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5171 - val_loss: 1.3328\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4011 - val_loss: 0.4217\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      " 71/242 [=======>......................] - ETA: 0s - loss: nanEpoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3363 - val_loss: 0.3211\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2954 - val_loss: 3.2635\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3063 - val_loss: 0.5937\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4273 - val_loss: 3.7486\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3703 - val_loss: 0.7367\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3321 - val_loss: 0.3689\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2945 - val_loss: 0.4566\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2980 - val_loss: 1.0407\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "142/242 [================>.............] - ETA: 0s - loss: nanEpoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3617 - val_loss: 0.4902\n",
      "Epoch 5/100\n",
      "121/121 [==============================] - 0s 986us/step - loss: 0.3189\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4245 - val_loss: 0.4098\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      " 44/242 [====>.........................] - ETA: 0s - loss: 0.3886Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 6/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2960 - val_loss: 0.3221\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3250 - val_loss: 0.4417\n",
      "Epoch 18/100\n",
      " 68/242 [=======>......................] - ETA: 0s - loss: 0.3266[CV] END .......learning_rate=0.05, n_hidden=2, n_neurons=60; total time=   5.5s\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3450 - val_loss: 0.3135\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3619 - val_loss: 9.3660\n",
      "231/242 [===========================>..] - ETA: 0s - loss: 0.3252Epoch 6/100\n",
      "242/242 [==============================] - 0s 974us/step - loss: nan - val_loss: nan\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3263 - val_loss: 0.3190\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2898 - val_loss: 0.3818\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3319 - val_loss: 0.3699\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3479 - val_loss: 0.3489\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3168 - val_loss: 0.3180\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2898 - val_loss: 0.3764\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6096 - val_loss: 0.6830\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 949us/step - loss: nan - val_loss: nan\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3243 - val_loss: 0.3503\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3891 - val_loss: 0.3016\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3193 - val_loss: 0.3121\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2883 - val_loss: 1.0305\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 944us/step - loss: nan - val_loss: nan\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4403 - val_loss: 3.1443\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3201 - val_loss: 1.0982\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3486 - val_loss: 0.3726\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3161 - val_loss: 0.3050\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3041 - val_loss: 3.6818\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3928 - val_loss: 0.6917\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3140 - val_loss: 0.3992\n",
      "Epoch 10/100\n",
      "121/121 [==============================] - 0s 368us/step - loss: nan\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      " 71/242 [=======>......................] - ETA: 0s - loss: 0.3149[CV] END ........learning_rate=0.5, n_hidden=1, n_neurons=60; total time=   3.3s\n",
      "Epoch 1/100\n",
      "121/121 [==============================] - 0s 1ms/step - loss: nan\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3237 - val_loss: 0.3298\n",
      "Epoch 10/100\n",
      "  1/242 [..............................] - ETA: 0s - loss: 0.2194[CV] END ........learning_rate=0.5, n_hidden=1, n_neurons=25; total time=   3.3s\n",
      "Epoch 1/100\n",
      "121/121 [==============================] - 0s 1ms/step - loss: nan\n",
      "121/121 [==============================] - 0s 993us/step - loss: nan\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3105 - val_loss: 0.3085\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3092 - val_loss: 1.4872\n",
      "Epoch 23/100\n",
      "  1/242 [..............................] - ETA: 0s - loss: 0.3551[CV] END ........learning_rate=0.5, n_hidden=1, n_neurons=60; total time=   3.6s\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3084 - val_loss: 0.4645\n",
      "Epoch 11/100\n",
      " 14/121 [==>...........................] - ETA: 0s - loss: nan[CV] END ........learning_rate=0.5, n_hidden=1, n_neurons=25; total time=   3.4s\n",
      "Epoch 1/100\n",
      "121/121 [==============================] - 0s 1ms/step - loss: nan\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3687 - val_loss: 0.3859\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - ETA: 0s - loss: 0.3124[CV] END ........learning_rate=0.5, n_hidden=1, n_neurons=25; total time=   3.5s\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3139 - val_loss: 0.3954\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3055 - val_loss: 0.3071\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3124 - val_loss: 0.3410\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3090 - val_loss: 0.5807\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 3.2063 - val_loss: 6.8084\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3536 - val_loss: 0.5294\n",
      "101/121 [========================>.....] - ETA: 0s - loss: 0.3540Epoch 6/100\n",
      "121/121 [==============================] - 0s 975us/step - loss: 0.3685\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3133 - val_loss: 0.3765\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3034 - val_loss: 0.3068\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 3.1351 - val_loss: 2.3027\n",
      "Epoch 2/100\n",
      "124/242 [==============>...............] - ETA: 0s - loss: 0.2914[CV] END .......learning_rate=0.05, n_hidden=2, n_neurons=60; total time=   7.7s\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 3.7537 - val_loss: 4.5912\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.4915 - val_loss: 3.7808\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 1.1201 - val_loss: 0.5641\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3008 - val_loss: 0.3102\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3505 - val_loss: 0.3772\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 833us/step - loss: 0.3011 - val_loss: 0.3054\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.9917 - val_loss: 0.8516\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.4184 - val_loss: 3.0830\n",
      "Epoch 3/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3021 - val_loss: 0.2864\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.6420 - val_loss: 3.4486\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2922 - val_loss: 0.3327\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.0461 - val_loss: 1.7105\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5389 - val_loss: 0.4761\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3399 - val_loss: 0.3574\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3003 - val_loss: 0.3087\n",
      "104/242 [===========>..................] - ETA: 0s - loss: 0.8841Epoch 27/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5686 - val_loss: 0.5308\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.0045 - val_loss: 2.9299\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2939 - val_loss: 0.2870\n",
      "  1/242 [..............................] - ETA: 0s - loss: 0.7577Epoch 14/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.9905 - val_loss: 2.0342\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2890 - val_loss: 0.3255\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8601 - val_loss: 0.9933\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 1.3224 - val_loss: 1.0611\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3334 - val_loss: 0.3095\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4688 - val_loss: 0.5005\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2988 - val_loss: 0.3085\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4930 - val_loss: 0.4436\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8491 - val_loss: 2.4122\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2941 - val_loss: 0.2906\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7814 - val_loss: 1.2234\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2940 - val_loss: 0.2850\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7697 - val_loss: 0.7610\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4386 - val_loss: 0.5277\n",
      " 63/242 [======>.......................] - ETA: 0s - loss: 0.7562Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6063 - val_loss: 2.4219\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3221 - val_loss: 0.3091\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2959 - val_loss: 0.2974\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7746 - val_loss: 1.9998\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2919 - val_loss: 0.3133\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4512 - val_loss: 0.4403\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7004 - val_loss: 0.8612\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7202 - val_loss: 0.6894\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5297 - val_loss: 0.8984\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4212 - val_loss: 0.4526\n",
      "  1/242 [..............................] - ETA: 0s - loss: 0.7006Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4765 - val_loss: 0.4270\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3169 - val_loss: 0.2881\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2877 - val_loss: 0.3328\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7308 - val_loss: 1.6685\n",
      "125/242 [==============>...............] - ETA: 0s - loss: 0.4827Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4281 - val_loss: 0.4771\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2870 - val_loss: 0.2838\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6880 - val_loss: 0.6446\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4772 - val_loss: 0.6045\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4094 - val_loss: 0.3848\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6608 - val_loss: 0.7054\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2834 - val_loss: 0.3775\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4256 - val_loss: 0.3716\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6988 - val_loss: 1.4129\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2798 - val_loss: 0.2976\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3143 - val_loss: 0.3882\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4129 - val_loss: 0.6243\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6630 - val_loss: 0.6184\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4002 - val_loss: 0.4423\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6368 - val_loss: 0.6264\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3579 - val_loss: 0.3361\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2811 - val_loss: 0.7825\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4733 - val_loss: 0.4655\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6724 - val_loss: 1.2319\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4037 - val_loss: 0.7816\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2807 - val_loss: 0.3045\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3036 - val_loss: 0.3286\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6417 - val_loss: 0.5971\n",
      "Epoch 13/100\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 971us/step - loss: 0.4237 - val_loss: 0.3903\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6193 - val_loss: 0.5952\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2768 - val_loss: 1.3241\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3598 - val_loss: 0.3839\n",
      "  1/242 [..............................] - ETA: 0s - loss: 0.4236Epoch 33/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3933 - val_loss: 0.3983\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6497 - val_loss: 1.0542\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3965 - val_loss: 0.8524\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2776 - val_loss: 0.2864\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3078 - val_loss: 0.2864\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6225 - val_loss: 0.5824\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3863 - val_loss: 0.4355\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4175 - val_loss: 0.3780\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3489 - val_loss: 0.3302\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2777 - val_loss: 0.2981\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3898 - val_loss: 1.2025\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6050 - val_loss: 0.5733\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6291 - val_loss: 0.9434\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2741 - val_loss: 0.2784\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3062 - val_loss: 0.3276\n",
      "233/242 [===========================>..] - ETA: 0s - loss: 0.3841Epoch 15/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6053 - val_loss: 0.5615\n",
      "Epoch 12/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3823 - val_loss: 0.3651\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4009 - val_loss: 0.3816\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5925 - val_loss: 0.5604\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2779 - val_loss: 0.3093\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.0723 - val_loss: 1.1078\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6109 - val_loss: 0.8191\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3857 - val_loss: 1.2227\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2721 - val_loss: 0.2901\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5891 - val_loss: 0.5449\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3030 - val_loss: 0.3678\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3763 - val_loss: 0.3574\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5810 - val_loss: 0.5481\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3925 - val_loss: 0.3771\n",
      "123/242 [==============>...............] - ETA: 0s - loss: 0.5647Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2718 - val_loss: 4.2662\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4705 - val_loss: 0.3943\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5941 - val_loss: 0.7063\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2673 - val_loss: 0.3400\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3812 - val_loss: 1.6691\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5742 - val_loss: 0.5337\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3125 - val_loss: 0.7717\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5705 - val_loss: 0.5379\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3731 - val_loss: 0.4422\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3890 - val_loss: 0.3638\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2710 - val_loss: 0.6992\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5788 - val_loss: 0.6339\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4185 - val_loss: 0.3831\n",
      "203/242 [========================>.....] - ETA: 0s - loss: 0.3640Epoch 37/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2709 - val_loss: 0.2685\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3785 - val_loss: 1.6263\n",
      "191/242 [======================>.......] - ETA: 0s - loss: 0.3770Epoch 13/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5603 - val_loss: 0.5183\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2967 - val_loss: 0.2832\n",
      "Epoch 15/100\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5606 - val_loss: 0.5284\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3701 - val_loss: 0.4222\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3813 - val_loss: 0.3814\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2689 - val_loss: 0.5361\n",
      "161/242 [==================>...........] - ETA: 0s - loss: 0.2779Epoch 25/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5646 - val_loss: 0.5842\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4657 - val_loss: 0.3570\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3753 - val_loss: 1.6885\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2675 - val_loss: 0.3070\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5472 - val_loss: 0.5075\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2871 - val_loss: 0.3015\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5514 - val_loss: 0.5195\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3671 - val_loss: 0.3511\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3763 - val_loss: 0.3540\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2665 - val_loss: 0.3117\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5516 - val_loss: 0.5439\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3969 - val_loss: 0.3603\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3724 - val_loss: 1.8118\n",
      "121/121 [==============================] - 0s 983us/step - loss: 0.3212\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5351 - val_loss: 0.5017\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2661 - val_loss: 0.3582\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5425 - val_loss: 0.5109\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3635 - val_loss: 0.3515\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2832 - val_loss: 0.2996\n",
      "Epoch 20/100\n",
      "218/242 [==========================>...] - ETA: 0s - loss: 0.5388[CV] END .......learning_rate=0.05, n_hidden=4, n_neurons=64; total time=   8.5s\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3697 - val_loss: 0.3474\n",
      "Epoch 14/100\n",
      "121/121 [==============================] - 0s 1ms/step - loss: 0.4100\n",
      "121/121 [==============================] - 0s 983us/step - loss: 0.3810\n",
      " 46/242 [====>.........................] - ETA: 0s - loss: 0.3744[CV] END ......learning_rate=0.005, n_hidden=2, n_neurons=26; total time=   5.1s\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5396 - val_loss: 0.5203\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5246 - val_loss: 0.4852\n",
      "Epoch 18/100\n",
      " 65/242 [=======>......................] - ETA: 0s - loss: 0.4734[CV] END .......learning_rate=0.05, n_hidden=2, n_neurons=60; total time=  12.5s\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5342 - val_loss: 0.5030\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3606 - val_loss: 0.4686\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2681 - val_loss: 0.2820\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2831 - val_loss: 0.2871\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3709 - val_loss: 0.3484\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5285 - val_loss: 0.5039\n",
      " 60/242 [======>.......................] - ETA: 0s - loss: nanEpoch 18/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5148 - val_loss: 0.4769\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2668 - val_loss: 0.2918\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5263 - val_loss: 0.4957\n",
      "Epoch 28/100\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3592 - val_loss: 0.3507\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3650 - val_loss: 0.3417\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2827 - val_loss: 0.2755\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5182 - val_loss: 0.4956\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 1s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5057 - val_loss: 0.4690\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3561 - val_loss: 0.3470\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2638 - val_loss: 0.4257\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5188 - val_loss: 0.4892\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2766 - val_loss: 0.2745\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3609 - val_loss: 0.3470\n",
      "Epoch 17/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5087 - val_loss: 0.4909\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "223/242 [==========================>...] - ETA: 0s - loss: 0.2649Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4975 - val_loss: 0.4618\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5116 - val_loss: 0.4832\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3535 - val_loss: 0.4131\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2622 - val_loss: 0.5169\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3605 - val_loss: 0.3695\n",
      " 62/242 [======>.......................] - ETA: 0s - loss: 0.2628Epoch 18/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2742 - val_loss: 0.2819\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5001 - val_loss: 0.4901\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3514 - val_loss: 0.3409\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5046 - val_loss: 0.4777\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4900 - val_loss: 0.4583\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2598 - val_loss: 0.4616\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2727 - val_loss: 0.4628\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3606 - val_loss: 0.3481\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4921 - val_loss: 0.4901\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4832 - val_loss: 0.4486\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4982 - val_loss: 0.4717\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3500 - val_loss: 0.4322\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2723 - val_loss: 0.2889\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3587 - val_loss: 0.3671\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2697 - val_loss: 0.2793\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4845 - val_loss: 0.4879\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4769 - val_loss: 0.4420\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4922 - val_loss: 0.4663\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3527 - val_loss: 0.3440\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2566 - val_loss: 0.2613\n",
      "  1/242 [..............................] - ETA: 0s - loss: 0.3529Epoch 33/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3491 - val_loss: 0.3379\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2668 - val_loss: 0.3084\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4775 - val_loss: 0.4899\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4710 - val_loss: 0.4382\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3467 - val_loss: 0.3407\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4863 - val_loss: 0.4621\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3499 - val_loss: 0.3517\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2558 - val_loss: 0.2685\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2665 - val_loss: 0.8454\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4711 - val_loss: 0.4869\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4658 - val_loss: 0.4322\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4808 - val_loss: 0.4558\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3452 - val_loss: 0.3354\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3475 - val_loss: 0.3648\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2578 - val_loss: 0.2779\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2657 - val_loss: 0.3171\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4651 - val_loss: 0.4851\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 976us/step - loss: nan - val_loss: nan\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4757 - val_loss: 0.4541\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3435 - val_loss: 0.3337\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4607 - val_loss: 0.4293\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3479 - val_loss: 0.3448\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2514 - val_loss: 0.2862\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2625 - val_loss: 0.2955\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4593 - val_loss: 0.4779\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4561 - val_loss: 0.4250\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "148/242 [=================>............] - ETA: 0s - loss: 0.2598Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3422 - val_loss: 0.4142\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4707 - val_loss: 0.4503\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3439 - val_loss: 0.3348\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2499 - val_loss: 0.3564\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2619 - val_loss: 0.3356\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4542 - val_loss: 0.4803\n",
      "Epoch 28/100\n",
      "121/121 [==============================] - 0s 955us/step - loss: nan\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4519 - val_loss: 0.4200\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4659 - val_loss: 0.4485\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3415 - val_loss: 0.3608\n",
      "Epoch 28/100\n",
      "237/242 [============================>.] - ETA: 0s - loss: 0.2635[CV] END ........learning_rate=0.5, n_hidden=2, n_neurons=78; total time=   3.7s\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3427 - val_loss: 0.3259\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2510 - val_loss: 0.2754\n",
      "Epoch 38/100\n",
      "121/121 [==============================] - 0s 981us/step - loss: nan\n",
      "121/121 [==============================] - 0s 1ms/step - loss: nan\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2628 - val_loss: 0.2735\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4493 - val_loss: 0.4715\n",
      "Epoch 29/100\n",
      "  1/242 [..............................] - ETA: 0s - loss: 0.4781[CV] END ........learning_rate=0.5, n_hidden=2, n_neurons=78; total time=   3.8s\n",
      "Epoch 1/100\n",
      "218/242 [==========================>...] - ETA: 0s - loss: 0.4609[CV] END ........learning_rate=0.5, n_hidden=2, n_neurons=78; total time=   3.6s\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4477 - val_loss: 0.4164\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4617 - val_loss: 0.4432\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3398 - val_loss: 0.3281\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3414 - val_loss: 0.3552\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2513 - val_loss: 0.2657\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2586 - val_loss: 0.3064\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4447 - val_loss: 0.4643\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 3.2777 - val_loss: 2.0704\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3388 - val_loss: 0.4017\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4576 - val_loss: 0.4419\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3395 - val_loss: 0.3353\n",
      "221/242 [==========================>...] - ETA: 0s - loss: 3.9160Epoch 28/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4442 - val_loss: 0.4140\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4403 - val_loss: 0.4606\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2531 - val_loss: 0.2529\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2475 - val_loss: 0.2806\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.1251 - val_loss: 1.6953\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 2.4530 - val_loss: 14.4249\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3384 - val_loss: 0.3223\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 3.7615 - val_loss: 3.8069\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4536 - val_loss: 0.4399\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3387 - val_loss: 0.3224\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4407 - val_loss: 0.4109\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4364 - val_loss: 0.4524\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2464 - val_loss: 0.2832\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2534 - val_loss: 0.2739\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8167 - val_loss: 0.9292\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.9945 - val_loss: 10.7952\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4500 - val_loss: 0.4381\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.5147 - val_loss: 3.8507\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3363 - val_loss: 0.3891\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3386 - val_loss: 0.3435\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4327 - val_loss: 0.4414\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4374 - val_loss: 0.4093\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2441 - val_loss: 0.2701\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2473 - val_loss: 0.2983\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4467 - val_loss: 0.4358\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.7021 - val_loss: 0.6654\n",
      "Epoch 33/100\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8290 - val_loss: 6.6918\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.0388 - val_loss: 2.0704\n",
      "215/242 [=========================>....] - ETA: 0s - loss: 0.4464Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3358 - val_loss: 0.3213\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3367 - val_loss: 0.3228\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4292 - val_loss: 0.4369\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2423 - val_loss: 0.2688\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4345 - val_loss: 0.4072\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2478 - val_loss: 0.2694\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4432 - val_loss: 0.4322\n",
      "Epoch 34/100\n",
      "121/121 [==============================] - 0s 1ms/step - loss: 0.2905\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3343 - val_loss: 0.3444\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8315 - val_loss: 1.3108\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3341 - val_loss: 0.3276\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6490 - val_loss: 0.5970\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7574 - val_loss: 4.0639\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4259 - val_loss: 0.4289\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4316 - val_loss: 0.4074\n",
      "112/242 [============>.................] - ETA: 0s - loss: 0.7363Epoch 35/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2493 - val_loss: 0.2550\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4402 - val_loss: 0.4323\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6177 - val_loss: 0.5733\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7176 - val_loss: 2.6430\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7281 - val_loss: 0.9153\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3358 - val_loss: 0.3188\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4228 - val_loss: 0.4247\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3328 - val_loss: 0.3880\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4289 - val_loss: 0.4015\n",
      "Epoch 36/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4372 - val_loss: 0.4301\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2460 - val_loss: 0.2742\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3338 - val_loss: 0.3438\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5943 - val_loss: 0.5601\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6868 - val_loss: 1.7571\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3319 - val_loss: 0.3299\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6683 - val_loss: 0.7297\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4200 - val_loss: 0.4128\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4263 - val_loss: 0.3986\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4347 - val_loss: 0.4296\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2454 - val_loss: 0.2628\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3327 - val_loss: 0.3181\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6300 - val_loss: 0.6374\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3315 - val_loss: 0.4521\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5746 - val_loss: 0.5448\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4173 - val_loss: 0.4058\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6605 - val_loss: 1.2210\n",
      " 80/242 [========>.....................] - ETA: 0s - loss: 0.3287Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4240 - val_loss: 0.3960\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4320 - val_loss: 0.4287\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2432 - val_loss: 0.2598\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3309 - val_loss: 0.3564\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5565 - val_loss: 0.5288\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3306 - val_loss: 0.3203\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6372 - val_loss: 0.8970\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4148 - val_loss: 0.4004\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6026 - val_loss: 0.5851\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4216 - val_loss: 0.3968\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4296 - val_loss: 0.4294\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3301 - val_loss: 0.3709\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2424 - val_loss: 0.2724\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 988us/step - loss: 0.5815 - val_loss: 0.5514\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3290 - val_loss: 0.3525\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5398 - val_loss: 0.5102\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6161 - val_loss: 0.7259\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4123 - val_loss: 0.3964\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4193 - val_loss: 0.3953\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4272 - val_loss: 0.4256\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3288 - val_loss: 0.3176\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2427 - val_loss: 0.2803\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3289 - val_loss: 0.3146\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5637 - val_loss: 0.5247\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5246 - val_loss: 0.4980\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4101 - val_loss: 0.3919\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5968 - val_loss: 0.6193\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4172 - val_loss: 0.3923\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4251 - val_loss: 0.4269\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 706us/step - loss: 0.4081 - val_loss: 0.3892\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3286 - val_loss: 0.3358\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2391 - val_loss: 0.3015\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5096 - val_loss: 0.4918\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3267 - val_loss: 0.3733\n",
      "  1/242 [..............................] - ETA: 0s - loss: 0.4290Epoch 41/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5493 - val_loss: 0.5127\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5787 - val_loss: 0.5714\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4151 - val_loss: 0.3944\n",
      "Epoch 42/100\n",
      "121/121 [==============================] - 0s 992us/step - loss: 0.3173\n",
      "242/242 [==============================] - 0s 986us/step - loss: 0.4060 - val_loss: 0.3870\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3276 - val_loss: 0.3174\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4228 - val_loss: 0.4318\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3267 - val_loss: 0.3303\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5363 - val_loss: 0.4981\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4967 - val_loss: 0.4700\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 935us/step - loss: 0.4041 - val_loss: 0.3865\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4131 - val_loss: 0.3908\n",
      "150/242 [=================>............] - ETA: 0s - loss: 0.3404Epoch 43/100\n",
      "242/242 [==============================] - 0s 610us/step - loss: 0.3270 - val_loss: 0.3312\n",
      "  1/242 [..............................] - ETA: 0s - loss: 0.7518Epoch 41/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5619 - val_loss: 0.5332\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 988us/step - loss: 0.3265 - val_loss: 0.3452\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5251 - val_loss: 0.4872\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4838 - val_loss: 0.4533\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4210 - val_loss: 0.4272\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4024 - val_loss: 0.3866\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4113 - val_loss: 0.3877\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3255 - val_loss: 0.3252\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5460 - val_loss: 0.5155\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3246 - val_loss: 0.3131\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4723 - val_loss: 0.4404\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5145 - val_loss: 0.4754\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4191 - val_loss: 0.4244\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4095 - val_loss: 0.3850\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4008 - val_loss: 0.3872\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3242 - val_loss: 0.3161\n",
      "Epoch 43/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5311 - val_loss: 0.5044\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3244 - val_loss: 0.3204\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4172 - val_loss: 0.4254\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4611 - val_loss: 0.4330\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5051 - val_loss: 0.4663\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4076 - val_loss: 0.3879\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3233 - val_loss: 0.3429\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3991 - val_loss: 0.3875\n",
      "Epoch 44/100\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5172 - val_loss: 0.4968\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3237 - val_loss: 0.3803\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4156 - val_loss: 0.4245\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4516 - val_loss: 0.4195\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4963 - val_loss: 0.4589\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4060 - val_loss: 0.3885\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3219 - val_loss: 0.3235\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3976 - val_loss: 0.3919\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5044 - val_loss: 0.4942\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 955us/step - loss: 0.3242 - val_loss: 0.3248\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 967us/step - loss: 0.4139 - val_loss: 0.4272\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 942us/step - loss: 0.4044 - val_loss: 0.3822\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4882 - val_loss: 0.4516\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 956us/step - loss: 0.3207 - val_loss: 0.3174\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 972us/step - loss: 0.3962 - val_loss: 0.3945\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4426 - val_loss: 0.4108\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 986us/step - loss: 0.3226 - val_loss: 0.5263\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 954us/step - loss: 0.4123 - val_loss: 0.4249\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4928 - val_loss: 0.4966\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 939us/step - loss: 0.4027 - val_loss: 0.3823\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 962us/step - loss: 0.3210 - val_loss: 0.3301\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 965us/step - loss: 0.3948 - val_loss: 0.3993\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4804 - val_loss: 0.4451\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4341 - val_loss: 0.4040\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3230 - val_loss: 0.3453\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 731us/step - loss: 0.3194 - val_loss: 0.3620\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4108 - val_loss: 0.4267\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 988us/step - loss: 0.4012 - val_loss: 0.3793\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4823 - val_loss: 0.4913\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 978us/step - loss: 0.3935 - val_loss: 0.4046\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4733 - val_loss: 0.4382\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4274 - val_loss: 0.3968\n",
      "145/242 [================>.............] - ETA: 0s - loss: 0.3891Epoch 21/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3211 - val_loss: 0.3419\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 922us/step - loss: 0.3188 - val_loss: 0.3411\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4094 - val_loss: 0.4309\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3923 - val_loss: 0.4165\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4727 - val_loss: 0.4800\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3999 - val_loss: 0.3833\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4664 - val_loss: 0.4323\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4208 - val_loss: 0.3910\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3199 - val_loss: 0.3062\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 934us/step - loss: 0.3184 - val_loss: 0.3383\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4078 - val_loss: 0.4225\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 1000us/step - loss: 0.3912 - val_loss: 0.4232\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 912us/step - loss: 0.3985 - val_loss: 0.3828\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4637 - val_loss: 0.4682\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4601 - val_loss: 0.4267\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 886us/step - loss: 0.3197 - val_loss: 0.3111\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3187 - val_loss: 0.3724\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 923us/step - loss: 0.4067 - val_loss: 0.4239\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4149 - val_loss: 0.3864\n",
      "  1/242 [..............................] - ETA: 0s - loss: 0.4462Epoch 23/100\n",
      "242/242 [==============================] - 0s 924us/step - loss: 0.3972 - val_loss: 0.3785\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 940us/step - loss: 0.3900 - val_loss: 0.4284\n",
      "242/242 [==============================] - 0s 901us/step - loss: 0.4558 - val_loss: 0.4657\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4541 - val_loss: 0.4213\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 958us/step - loss: 0.3170 - val_loss: 0.3149\n",
      "Epoch 52/100\n",
      "121/121 [==============================] - 0s 622us/step - loss: 0.3931\n",
      "242/242 [==============================] - 0s 911us/step - loss: 0.4054 - val_loss: 0.4224\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 892us/step - loss: 0.3958 - val_loss: 0.3770\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4093 - val_loss: 0.3847\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 906us/step - loss: 0.4484 - val_loss: 0.4597\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3184 - val_loss: 0.3305\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 911us/step - loss: 0.4486 - val_loss: 0.4168\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 823us/step - loss: 0.3162 - val_loss: 0.3182\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 825us/step - loss: 0.4041 - val_loss: 0.4268\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 820us/step - loss: 0.3948 - val_loss: 0.3797\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 843us/step - loss: 0.3180 - val_loss: 0.3358\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 932us/step - loss: 0.4047 - val_loss: 0.3854\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4415 - val_loss: 0.4470\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 822us/step - loss: 0.3166 - val_loss: 0.3544\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 929us/step - loss: 0.4432 - val_loss: 0.4134\n",
      "Epoch 25/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 813us/step - loss: 0.4030 - val_loss: 0.4251\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 817us/step - loss: 0.3935 - val_loss: 0.3815\n",
      "155/242 [==================>...........] - ETA: 0s - loss: 0.4179Epoch 56/100\n",
      "242/242 [==============================] - 0s 828us/step - loss: 0.3161 - val_loss: 0.3054\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 945us/step - loss: 0.4002 - val_loss: 0.3832\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 857us/step - loss: 0.3138 - val_loss: 0.3128\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4358 - val_loss: 0.4381\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 967us/step - loss: 0.4382 - val_loss: 0.4110\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 876us/step - loss: 0.4019 - val_loss: 0.4241\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 873us/step - loss: 0.3923 - val_loss: 0.3821\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 925us/step - loss: 0.3161 - val_loss: 0.3031\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 985us/step - loss: 0.3961 - val_loss: 0.3837\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3140 - val_loss: 0.3299\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4300 - val_loss: 0.4259\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 989us/step - loss: 0.4334 - val_loss: 0.4063\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 883us/step - loss: 0.4007 - val_loss: 0.4263\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 883us/step - loss: 0.3911 - val_loss: 0.3787\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 875us/step - loss: 0.3159 - val_loss: 0.3765\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 980us/step - loss: 0.3923 - val_loss: 0.3878\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 948us/step - loss: 0.3128 - val_loss: 0.3222\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 882us/step - loss: 0.3996 - val_loss: 0.4192\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 995us/step - loss: 0.4250 - val_loss: 0.4153\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 889us/step - loss: 0.3901 - val_loss: 0.3811\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4291 - val_loss: 0.4021\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 864us/step - loss: 0.3148 - val_loss: 0.3119\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 946us/step - loss: 0.3890 - val_loss: 0.3802\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 897us/step - loss: 0.3114 - val_loss: 0.3466\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 865us/step - loss: 0.3987 - val_loss: 0.4211\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 859us/step - loss: 0.3890 - val_loss: 0.3751\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 960us/step - loss: 0.4203 - val_loss: 0.4094\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 952us/step - loss: 0.4248 - val_loss: 0.4006\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 861us/step - loss: 0.3131 - val_loss: 0.3117\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 975us/step - loss: 0.3860 - val_loss: 0.3821\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 858us/step - loss: 0.3109 - val_loss: 0.3195\n",
      "220/242 [==========================>...] - ETA: 0s - loss: 0.4244Epoch 59/100\n",
      "242/242 [==============================] - 0s 873us/step - loss: 0.3976 - val_loss: 0.4241\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 874us/step - loss: 0.3879 - val_loss: 0.3764\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4160 - val_loss: 0.3980\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4208 - val_loss: 0.4032\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 926us/step - loss: 0.3135 - val_loss: 0.3588\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 869us/step - loss: 0.3109 - val_loss: 0.3126\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 914us/step - loss: 0.3831 - val_loss: 0.3901\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 881us/step - loss: 0.3966 - val_loss: 0.4236\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 863us/step - loss: 0.3869 - val_loss: 0.3808\n",
      "224/242 [==========================>...] - ETA: 0s - loss: 0.4087Epoch 62/100\n",
      "242/242 [==============================] - 0s 858us/step - loss: 0.3123 - val_loss: 0.3292\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 963us/step - loss: 0.4119 - val_loss: 0.3907\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 966us/step - loss: 0.4175 - val_loss: 0.3950\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 875us/step - loss: 0.3097 - val_loss: 0.3160\n",
      "242/242 [==============================] - 0s 915us/step - loss: 0.3957 - val_loss: 0.4216\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 988us/step - loss: 0.3804 - val_loss: 0.3941\n",
      "242/242 [==============================] - 0s 879us/step - loss: 0.3858 - val_loss: 0.3706\n",
      "Epoch 32/100\n",
      "Epoch 63/100\n",
      "121/121 [==============================] - 0s 554us/step - loss: 0.3122\n",
      "242/242 [==============================] - 0s 898us/step - loss: 0.3124 - val_loss: 0.3420\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 957us/step - loss: 0.4080 - val_loss: 0.3863\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 973us/step - loss: 0.4137 - val_loss: 0.3962\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 770us/step - loss: 0.3849 - val_loss: 0.3817\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 873us/step - loss: 0.3947 - val_loss: 0.4257\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 859us/step - loss: 0.3780 - val_loss: 0.3916\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 708us/step - loss: 0.3111 - val_loss: 0.3519\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 830us/step - loss: 0.4043 - val_loss: 0.3818\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 819us/step - loss: 0.4102 - val_loss: 0.3895\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 737us/step - loss: 0.3839 - val_loss: 0.3796\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 788us/step - loss: 0.3939 - val_loss: 0.4278\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 866us/step - loss: 0.3758 - val_loss: 0.3934\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 766us/step - loss: 0.3108 - val_loss: 0.3142\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 862us/step - loss: 0.4008 - val_loss: 0.3836\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 870us/step - loss: 0.4076 - val_loss: 0.3907\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 765us/step - loss: 0.3829 - val_loss: 0.3835\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 801us/step - loss: 0.3929 - val_loss: 0.4283\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 775us/step - loss: 0.3736 - val_loss: 0.3906\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 772us/step - loss: 0.3104 - val_loss: 0.2990\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 912us/step - loss: 0.3979 - val_loss: 0.3823\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 923us/step - loss: 0.4046 - val_loss: 0.3915\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 834us/step - loss: 0.3822 - val_loss: 0.3731\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 821us/step - loss: 0.3922 - val_loss: 0.4217\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 824us/step - loss: 0.3094 - val_loss: 0.3542\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 911us/step - loss: 0.3718 - val_loss: 0.3959\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 790us/step - loss: 0.3812 - val_loss: 0.3837\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 941us/step - loss: 0.3946 - val_loss: 0.3840\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 893us/step - loss: 0.4018 - val_loss: 0.3874\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 805us/step - loss: 0.3913 - val_loss: 0.4172\n",
      "Epoch 67/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 796us/step - loss: 0.3090 - val_loss: 0.3261\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 850us/step - loss: 0.3699 - val_loss: 0.3860\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 809us/step - loss: 0.3804 - val_loss: 0.3802\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 902us/step - loss: 0.3913 - val_loss: 0.3818\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 908us/step - loss: 0.3991 - val_loss: 0.3874\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 799us/step - loss: 0.3090 - val_loss: 0.2966\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 947us/step - loss: 0.3905 - val_loss: 0.4201\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 824us/step - loss: 0.3684 - val_loss: 0.3915\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 786us/step - loss: 0.3794 - val_loss: 0.3672\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 877us/step - loss: 0.3888 - val_loss: 0.3929\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 860us/step - loss: 0.3964 - val_loss: 0.3858\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 800us/step - loss: 0.3897 - val_loss: 0.4208\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 856us/step - loss: 0.3079 - val_loss: 0.2980\n",
      "  1/242 [..............................] - ETA: 0s - loss: 0.5698Epoch 69/100\n",
      "242/242 [==============================] - 0s 888us/step - loss: 0.3667 - val_loss: 0.3971\n",
      "242/242 [==============================] - 0s 830us/step - loss: 0.3788 - val_loss: 0.3695\n",
      "Epoch 71/100\n",
      "121/121 [==============================] - 0s 535us/step - loss: 0.3853\n",
      "242/242 [==============================] - 0s 890us/step - loss: 0.3858 - val_loss: 0.4133\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 887us/step - loss: 0.3941 - val_loss: 0.3861\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 799us/step - loss: 0.3073 - val_loss: 0.3002\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 855us/step - loss: 0.3889 - val_loss: 0.4155\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 721us/step - loss: 0.3780 - val_loss: 0.3715\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 677us/step - loss: 0.3062 - val_loss: 0.4158\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 674us/step - loss: 0.3881 - val_loss: 0.4142\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 782us/step - loss: 0.3833 - val_loss: 0.4187\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 781us/step - loss: 0.3918 - val_loss: 0.3818\n",
      "  1/242 [..............................] - ETA: 0s - loss: 0.3742Epoch 40/100\n",
      "242/242 [==============================] - 0s 662us/step - loss: 0.3771 - val_loss: 0.3777\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 655us/step - loss: 0.3068 - val_loss: 0.2968\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 666us/step - loss: 0.3874 - val_loss: 0.4151\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 765us/step - loss: 0.3894 - val_loss: 0.3801\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 777us/step - loss: 0.3808 - val_loss: 0.4233\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 652us/step - loss: 0.3764 - val_loss: 0.3671\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 657us/step - loss: 0.3060 - val_loss: 0.4192\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 656us/step - loss: 0.3866 - val_loss: 0.4182\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 747us/step - loss: 0.3875 - val_loss: 0.3800\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 745us/step - loss: 0.3785 - val_loss: 0.4459\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 649us/step - loss: 0.3757 - val_loss: 0.3687\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 649us/step - loss: 0.3062 - val_loss: 0.2962\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 658us/step - loss: 0.3860 - val_loss: 0.4131\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 764us/step - loss: 0.3852 - val_loss: 0.3835\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 654us/step - loss: 0.3750 - val_loss: 0.3691\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 771us/step - loss: 0.3762 - val_loss: 0.4571\n",
      "  1/242 [..............................] - ETA: 0s - loss: 0.3453Epoch 42/100\n",
      "242/242 [==============================] - 0s 655us/step - loss: 0.3039 - val_loss: 0.2936\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 653us/step - loss: 0.3853 - val_loss: 0.4138\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 659us/step - loss: 0.3742 - val_loss: 0.3750\n",
      "230/242 [===========================>..] - ETA: 0s - loss: 0.3049Epoch 77/100\n",
      "242/242 [==============================] - 0s 753us/step - loss: 0.3835 - val_loss: 0.3829\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 758us/step - loss: 0.3741 - val_loss: 0.4627\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 657us/step - loss: 0.3028 - val_loss: 0.4050\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 665us/step - loss: 0.3847 - val_loss: 0.4117\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 652us/step - loss: 0.3736 - val_loss: 0.3810\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 764us/step - loss: 0.3816 - val_loss: 0.3796\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 750us/step - loss: 0.3720 - val_loss: 0.4930\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 652us/step - loss: 0.3057 - val_loss: 0.4715\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 651us/step - loss: 0.3840 - val_loss: 0.4105\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 657us/step - loss: 0.3729 - val_loss: 0.3728\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 648us/step - loss: 0.3051 - val_loss: 0.4509\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 741us/step - loss: 0.3795 - val_loss: 0.3769\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 750us/step - loss: 0.3701 - val_loss: 0.5074\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 656us/step - loss: 0.3833 - val_loss: 0.4106\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 677us/step - loss: 0.3723 - val_loss: 0.3758\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 670us/step - loss: 0.3040 - val_loss: 0.2950\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 668us/step - loss: 0.3826 - val_loss: 0.4111\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 770us/step - loss: 0.3779 - val_loss: 0.3747\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 773us/step - loss: 0.3680 - val_loss: 0.5030\n",
      "242/242 [==============================] - 0s 674us/step - loss: 0.3717 - val_loss: 0.3661\n",
      "Epoch 81/100\n",
      "121/121 [==============================] - 0s 451us/step - loss: 0.3779\n",
      "242/242 [==============================] - 0s 654us/step - loss: 0.3005 - val_loss: 0.4438\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 649us/step - loss: 0.3821 - val_loss: 0.4148\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 738us/step - loss: 0.3762 - val_loss: 0.3742\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 610us/step - loss: 0.3707 - val_loss: 0.3879\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 618us/step - loss: 0.3020 - val_loss: 0.4673\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 616us/step - loss: 0.3815 - val_loss: 0.4170\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 721us/step - loss: 0.3745 - val_loss: 0.3786\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 635us/step - loss: 0.3705 - val_loss: 0.3725\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 640us/step - loss: 0.3015 - val_loss: 0.4077\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 663us/step - loss: 0.3809 - val_loss: 0.4121\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 654us/step - loss: 0.3698 - val_loss: 0.3745\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 760us/step - loss: 0.3730 - val_loss: 0.3807\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 635us/step - loss: 0.3008 - val_loss: 0.5163\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 638us/step - loss: 0.3803 - val_loss: 0.4158\n",
      "Epoch 83/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 642us/step - loss: 0.3692 - val_loss: 0.3748\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 741us/step - loss: 0.3712 - val_loss: 0.3720\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 623us/step - loss: 0.3004 - val_loss: 0.4683\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 627us/step - loss: 0.3798 - val_loss: 0.4132\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 613us/step - loss: 0.3686 - val_loss: 0.3633\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 618us/step - loss: 0.3020 - val_loss: 0.3973\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.3696 - val_loss: 0.3799\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 625us/step - loss: 0.3791 - val_loss: 0.4052\n",
      "Epoch 85/100\n",
      "121/121 [==============================] - 0s 378us/step - loss: 0.3306\n",
      "242/242 [==============================] - 0s 620us/step - loss: 0.3680 - val_loss: 0.3676\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 584us/step - loss: 0.3787 - val_loss: 0.4114\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 683us/step - loss: 0.3683 - val_loss: 0.3706\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 575us/step - loss: 0.3675 - val_loss: 0.3625\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 572us/step - loss: 0.3780 - val_loss: 0.4066\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 667us/step - loss: 0.3665 - val_loss: 0.3616\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 572us/step - loss: 0.3670 - val_loss: 0.3639\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 567us/step - loss: 0.3777 - val_loss: 0.4061\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 669us/step - loss: 0.3656 - val_loss: 0.3757\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 571us/step - loss: 0.3664 - val_loss: 0.3709\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 564us/step - loss: 0.3772 - val_loss: 0.4037\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 569us/step - loss: 0.3658 - val_loss: 0.3731\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 669us/step - loss: 0.3641 - val_loss: 0.3784\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 586us/step - loss: 0.3766 - val_loss: 0.4003\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 576us/step - loss: 0.3653 - val_loss: 0.3609\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.3628 - val_loss: 0.3702\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 566us/step - loss: 0.3761 - val_loss: 0.4012\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 568us/step - loss: 0.3648 - val_loss: 0.3576\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 662us/step - loss: 0.3616 - val_loss: 0.3763\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 553us/step - loss: 0.3756 - val_loss: 0.4072\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 569us/step - loss: 0.3643 - val_loss: 0.3673\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 568us/step - loss: 0.3752 - val_loss: 0.4092\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 666us/step - loss: 0.3605 - val_loss: 0.3769\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 566us/step - loss: 0.3637 - val_loss: 0.3592\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 549us/step - loss: 0.3747 - val_loss: 0.4127\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 658us/step - loss: 0.3590 - val_loss: 0.3805\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 567us/step - loss: 0.3633 - val_loss: 0.3594\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 558us/step - loss: 0.3742 - val_loss: 0.4057\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 669us/step - loss: 0.3580 - val_loss: 0.3738\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 580us/step - loss: 0.3628 - val_loss: 0.3688\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 585us/step - loss: 0.3738 - val_loss: 0.4061\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 584us/step - loss: 0.3623 - val_loss: 0.3584\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.3566 - val_loss: 0.3741\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 589us/step - loss: 0.3733 - val_loss: 0.4074\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 611us/step - loss: 0.3618 - val_loss: 0.3705\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 701us/step - loss: 0.3556 - val_loss: 0.3752\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 609us/step - loss: 0.3729 - val_loss: 0.4055\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 596us/step - loss: 0.3613 - val_loss: 0.3596\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 696us/step - loss: 0.3544 - val_loss: 0.3548\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 599us/step - loss: 0.3724 - val_loss: 0.4047\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 587us/step - loss: 0.3608 - val_loss: 0.3635\n",
      "242/242 [==============================] - 0s 561us/step - loss: 0.3719 - val_loss: 0.4114\n",
      "121/121 [==============================] - 0s 365us/step - loss: 0.3821\n",
      "242/242 [==============================] - 0s 668us/step - loss: 0.3531 - val_loss: 0.3565\n",
      "Epoch 65/100\n",
      "121/121 [==============================] - 0s 333us/step - loss: 0.3740\n",
      "242/242 [==============================] - 0s 584us/step - loss: 0.3525 - val_loss: 0.3613\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 570us/step - loss: 0.3515 - val_loss: 0.3637\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 590us/step - loss: 0.3505 - val_loss: 0.3638\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 536us/step - loss: 0.3496 - val_loss: 0.3653\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 538us/step - loss: 0.3488 - val_loss: 0.3498\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 558us/step - loss: 0.3479 - val_loss: 0.3567\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 571us/step - loss: 0.3467 - val_loss: 0.3713\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 552us/step - loss: 0.3460 - val_loss: 0.3516\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 555us/step - loss: 0.3454 - val_loss: 0.3573\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 572us/step - loss: 0.3443 - val_loss: 0.3667\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 583us/step - loss: 0.3436 - val_loss: 0.3581\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 560us/step - loss: 0.3428 - val_loss: 0.3705\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 554us/step - loss: 0.3422 - val_loss: 0.3529\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 559us/step - loss: 0.3412 - val_loss: 0.3530\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 550us/step - loss: 0.3405 - val_loss: 0.3606\n",
      "121/121 [==============================] - 0s 315us/step - loss: 0.3388\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hk/miniconda3/lib/python3.9/site-packages/sklearn/model_selection/_search.py:953: UserWarning: One or more of the test scores are non-finite: [        nan -0.35615668         nan         nan         nan -0.30967164\n",
      " -0.38306371 -0.35094832         nan -0.36732407]\n",
      "  warnings.warn(\n",
      "/Users/hk/miniconda3/lib/python3.9/site-packages/keras/optimizers/optimizer_v2/gradient_descent.py:111: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "363/363 [==============================] - 0s 690us/step - loss: 0.5500 - val_loss: 0.4232\n",
      "Epoch 2/100\n",
      "363/363 [==============================] - 0s 496us/step - loss: 0.3853 - val_loss: 0.8073\n",
      "Epoch 3/100\n",
      "363/363 [==============================] - 0s 513us/step - loss: 0.3700 - val_loss: 0.3326\n",
      "Epoch 4/100\n",
      "363/363 [==============================] - 0s 493us/step - loss: 0.3459 - val_loss: 0.3433\n",
      "Epoch 5/100\n",
      "363/363 [==============================] - 0s 490us/step - loss: 0.3341 - val_loss: 0.3431\n",
      "Epoch 6/100\n",
      "363/363 [==============================] - 0s 558us/step - loss: 0.3252 - val_loss: 0.5594\n",
      "Epoch 7/100\n",
      "363/363 [==============================] - 0s 514us/step - loss: 0.3214 - val_loss: 0.4342\n",
      "Epoch 8/100\n",
      "363/363 [==============================] - 0s 515us/step - loss: 0.3098 - val_loss: 1.8370\n",
      "Epoch 9/100\n",
      "363/363 [==============================] - 0s 502us/step - loss: 0.3004 - val_loss: 0.3659\n",
      "Epoch 10/100\n",
      "363/363 [==============================] - 0s 523us/step - loss: 0.2956 - val_loss: 0.3432\n",
      "Epoch 11/100\n",
      "363/363 [==============================] - 0s 501us/step - loss: 0.2939 - val_loss: 0.4256\n",
      "Epoch 12/100\n",
      "363/363 [==============================] - 0s 505us/step - loss: 0.2878 - val_loss: 0.2953\n",
      "Epoch 13/100\n",
      "363/363 [==============================] - 0s 525us/step - loss: 0.2843 - val_loss: 0.2791\n",
      "Epoch 14/100\n",
      "363/363 [==============================] - 0s 541us/step - loss: 0.2813 - val_loss: 0.2742\n",
      "Epoch 15/100\n",
      "363/363 [==============================] - 0s 518us/step - loss: 0.2812 - val_loss: 0.3568\n",
      "Epoch 16/100\n",
      "363/363 [==============================] - 0s 509us/step - loss: 0.2772 - val_loss: 0.2672\n",
      "Epoch 17/100\n",
      "363/363 [==============================] - 0s 509us/step - loss: 0.2743 - val_loss: 0.5766\n",
      "Epoch 18/100\n",
      "363/363 [==============================] - 0s 486us/step - loss: 0.2727 - val_loss: 0.3032\n",
      "Epoch 19/100\n",
      "363/363 [==============================] - 0s 519us/step - loss: 0.2721 - val_loss: 0.2607\n",
      "Epoch 20/100\n",
      "363/363 [==============================] - 0s 501us/step - loss: 0.2664 - val_loss: 0.3127\n",
      "Epoch 21/100\n",
      "363/363 [==============================] - 0s 475us/step - loss: 0.2621 - val_loss: 0.2914\n",
      "Epoch 22/100\n",
      "363/363 [==============================] - 0s 485us/step - loss: 0.2595 - val_loss: 0.5373\n",
      "Epoch 23/100\n",
      "363/363 [==============================] - 0s 489us/step - loss: 0.2632 - val_loss: 0.5288\n",
      "Epoch 24/100\n",
      "363/363 [==============================] - 0s 494us/step - loss: 0.2624 - val_loss: 0.2955\n",
      "Epoch 25/100\n",
      "363/363 [==============================] - 0s 504us/step - loss: 0.2597 - val_loss: 0.2789\n",
      "Epoch 26/100\n",
      "363/363 [==============================] - 0s 513us/step - loss: 0.2541 - val_loss: 0.2686\n",
      "Epoch 27/100\n",
      "363/363 [==============================] - 0s 508us/step - loss: 0.2532 - val_loss: 0.2721\n",
      "Epoch 28/100\n",
      "363/363 [==============================] - 0s 522us/step - loss: 0.2507 - val_loss: 0.2828\n",
      "Epoch 29/100\n",
      "363/363 [==============================] - 0s 513us/step - loss: 0.2502 - val_loss: 0.2562\n",
      "Epoch 30/100\n",
      "363/363 [==============================] - 0s 525us/step - loss: 0.2500 - val_loss: 0.2665\n",
      "Epoch 31/100\n",
      "363/363 [==============================] - 0s 471us/step - loss: 0.2470 - val_loss: 0.2863\n",
      "Epoch 32/100\n",
      "363/363 [==============================] - 0s 479us/step - loss: 0.2435 - val_loss: 0.2917\n",
      "Epoch 33/100\n",
      "363/363 [==============================] - 0s 502us/step - loss: 0.2436 - val_loss: 0.2611\n",
      "Epoch 34/100\n",
      "363/363 [==============================] - 0s 486us/step - loss: 0.2410 - val_loss: 0.2690\n",
      "Epoch 35/100\n",
      "363/363 [==============================] - 0s 497us/step - loss: 0.2394 - val_loss: 0.2921\n",
      "Epoch 36/100\n",
      "363/363 [==============================] - 0s 517us/step - loss: 0.2393 - val_loss: 0.2735\n",
      "Epoch 37/100\n",
      "363/363 [==============================] - 0s 493us/step - loss: 0.2415 - val_loss: 0.3196\n",
      "Epoch 38/100\n",
      "363/363 [==============================] - 0s 517us/step - loss: 0.2345 - val_loss: 0.2707\n",
      "Epoch 39/100\n",
      "363/363 [==============================] - 0s 531us/step - loss: 0.2363 - val_loss: 0.2847\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomizedSearchCV(cv=3,\n",
       "                   estimator=&lt;keras.wrappers.scikit_learn.KerasRegressor object at 0x1079e0430&gt;,\n",
       "                   n_jobs=-1,\n",
       "                   param_distributions={&#x27;learning_rate&#x27;: [0.0005, 0.005, 0.05,\n",
       "                                                          0.5],\n",
       "                                        &#x27;n_hidden&#x27;: [1, 2, 3, 4],\n",
       "                                        &#x27;n_neurons&#x27;: [10, 11, 12, 13, 14, 15,\n",
       "                                                      16, 17, 18, 19, 20, 21,\n",
       "                                                      22, 23, 24, 25, 26, 27,\n",
       "                                                      28, 29, 30, 31, 32, 33,\n",
       "                                                      34, 35, 36, 37, 38, 39, ...]},\n",
       "                   verbose=2)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomizedSearchCV</label><div class=\"sk-toggleable__content\"><pre>RandomizedSearchCV(cv=3,\n",
       "                   estimator=&lt;keras.wrappers.scikit_learn.KerasRegressor object at 0x1079e0430&gt;,\n",
       "                   n_jobs=-1,\n",
       "                   param_distributions={&#x27;learning_rate&#x27;: [0.0005, 0.005, 0.05,\n",
       "                                                          0.5],\n",
       "                                        &#x27;n_hidden&#x27;: [1, 2, 3, 4],\n",
       "                                        &#x27;n_neurons&#x27;: [10, 11, 12, 13, 14, 15,\n",
       "                                                      16, 17, 18, 19, 20, 21,\n",
       "                                                      22, 23, 24, 25, 26, 27,\n",
       "                                                      28, 29, 30, 31, 32, 33,\n",
       "                                                      34, 35, 36, 37, 38, 39, ...]},\n",
       "                   verbose=2)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: KerasRegressor</label><div class=\"sk-toggleable__content\"><pre>&lt;keras.wrappers.scikit_learn.KerasRegressor object at 0x1079e0430&gt;</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KerasRegressor</label><div class=\"sk-toggleable__content\"><pre>&lt;keras.wrappers.scikit_learn.KerasRegressor object at 0x1079e0430&gt;</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomizedSearchCV(cv=3,\n",
       "                   estimator=<keras.wrappers.scikit_learn.KerasRegressor object at 0x1079e0430>,\n",
       "                   n_jobs=-1,\n",
       "                   param_distributions={'learning_rate': [0.0005, 0.005, 0.05,\n",
       "                                                          0.5],\n",
       "                                        'n_hidden': [1, 2, 3, 4],\n",
       "                                        'n_neurons': [10, 11, 12, 13, 14, 15,\n",
       "                                                      16, 17, 18, 19, 20, 21,\n",
       "                                                      22, 23, 24, 25, 26, 27,\n",
       "                                                      28, 29, 30, 31, 32, 33,\n",
       "                                                      34, 35, 36, 37, 38, 39, ...]},\n",
       "                   verbose=2)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Define hyperparameter sets and ranges to explore\n",
    "param_distribs = {\n",
    "    \"n_hidden\": [1, 2, 3, 4],\n",
    "    \"n_neurons\": list(range(10, 100)),\n",
    "    \"learning_rate\": [5e-4, 5e-3, 5e-2, 5e-1]    #default learning rate is 1e-2\n",
    "}\n",
    "                      \n",
    "# Create an instance of RandomizedSearchCV\n",
    "rnd_search_cv = RandomizedSearchCV(keras_reg, param_distribs, n_iter=10, cv=3, n_jobs=-1, verbose=2)\n",
    "\n",
    "# Search\n",
    "rnd_search_cv.fit(X_train, y_train, epochs=100,\n",
    "                  validation_data=(X_valid, y_valid),\n",
    "                  callbacks=[keras.callbacks.EarlyStopping(patience=10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_neurons': 64, 'n_hidden': 4, 'learning_rate': 0.05}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the parameters of the best model.\n",
    "rnd_search_cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162/162 [==============================] - 0s 378us/step - loss: 0.2692\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5188452414900341"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the model for the best estimator, and evaluate it on the test set (outputs the MSE).\n",
    "\n",
    "best_model = rnd_search_cv.best_estimator_.model\n",
    "loss = best_model.evaluate(X_test, y_test)\n",
    "\n",
    "# Compute and display RMSE\n",
    "np.sqrt(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note\n",
    "There are many alternative techniques that can explore a search space more efficient than RandomSerarchCV. There are a list of libraries in the book on pages 322-333.\n",
    "\n",
    "I have tried BayesSearchCV, but results have been dissapointing so far."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow Serving\n",
    "\n",
    "You can deploy a model as a REST API using TensorFlow Serving (TF Serving)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: housing_model/03/assets\n"
     ]
    }
   ],
   "source": [
    "# Before you start TF Serving, you should export the model to TensorFlow's SavedModel format.\n",
    "model_version = \"03\"\n",
    "model_name = \"housing_model\"\n",
    "model_path = os.path.join(model_name, model_version)\n",
    "tf.saved_model.save(model, model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, I will install TF Serving using a Docker image (there are also other options).\n",
    "\n",
    "First, you should pull the Docker image by typing the following command from the command prompt:\n",
    "\n",
    "docker pull tensorflow/serving\n",
    "\n",
    "Then you can run the docker image by typing a command similar to the following:\n",
    "\n",
    "docker run -it --rm -p 8500:8500 -p 8501:8501 -v \"/Users/hk/Documents/Undervisning/ML/Examples/housing_model:/models/housing_model\" -e MODEL_NAME=housing_model tensorflow/serving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"signature_name\": \"serving_default\", \"instances\": [[-1.157801044672084, -0.28673138272452353, -0.49550876553420803, -0.16618096584447373, -0.029460121855354803, 0.38899735425767396, 0.19374820779179167, 0.28704739633254056], [-0.71255310326722, 0.10880952094123046, -0.16332972890163902, 0.20164651866470046, 0.12842116621683547, -0.11818173761799093, -0.237252610085982, 0.062152314019760624], [-0.21561010032058425, 1.849189497070548, -0.5798278760656232, 0.18528489288618866, -0.10429402718436985, -0.6769490515190195, 1.0089019285606207, -1.4271528977404662]]}'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You can make a prediction by querying TF Serving REST API. A query must be a POST request,\n",
    "# and the input data must be passed in the request body as a JSON object.\n",
    "\n",
    "# (I will make the request in Postman)\n",
    "\n",
    "import json\n",
    "\n",
    "input_data_json = json.dumps({\n",
    "    \"signature_name\": \"serving_default\",\n",
    "    \"instances\": X_new.tolist()\n",
    "})\n",
    "\n",
    "input_data_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "nav_menu": {
   "height": "264px",
   "width": "369px"
  },
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
