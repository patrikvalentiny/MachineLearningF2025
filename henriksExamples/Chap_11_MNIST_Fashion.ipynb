{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP hyperparameter tuning using the Fashion MNIST dataset\n",
    "We will train and evaluate an MLP on the Fashion MNIST dataset. It consists of 70.000 grayscale images of 28x28 pixels each, and there are 10 classes.\n",
    "\n",
    "Hyperparameters:\n",
    "1. Optimizer: Momentum optimization\n",
    "2. Activation function: SELU\n",
    "3. Weight initialization: LeCun\n",
    "4. Learning rate schedule: Performance scheduling\n",
    "5. Regularization: Alpha dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common imports\n",
    "import sys\n",
    "import os\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ignore useless warnings (see SciPy issue #5998)\n",
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\", message=\"^internal gelsd\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the full training set into a validation set and a (smaller) training set.\n",
    "# Normalize the input data to a range from 0 to 1.\n",
    "X_valid, X_train = X_train_full[:5000], X_train_full[5000:]\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardize the data\n",
    "Because we want to use the SELU activation function and LeCun weight initializer, we should standardize all the input features to a mean of 0 and a standard deviation of 1. Since each pixel is an input feature, there are 28x28=784 input features, and we need to compute the mean and standard deviation for each of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the mean for each pixel.\n",
    "pixel_means = X_train.mean(axis=0)\n",
    "pixel_means.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the standard deviation for each pixel.\n",
    "pixel_stds = X_train.std(axis=0)\n",
    "pixel_stds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the inputs to mean 0 and standard deviation 1 to achieve self-normalization with SELU and LeCun.\n",
    "X_train_standardized = (X_train - pixel_means) / pixel_stds\n",
    "\n",
    "X_valid_standardized = (X_valid - X_valid.mean(axis=0)) / X_valid.std(axis=0)\n",
    "X_test_standardized = (X_test - X_test.mean(axis=0)) / X_test.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate that the mean is close to 0 for each pixel.\n",
    "X_train_standardized.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate that the standard deviation is close to 1 for each pixel.\n",
    "X_train_standardized.std(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a model using the Sequential API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "# Input layer:\n",
    "model.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
    "\n",
    "# Hidden layers:\n",
    "for layer in range(3):\n",
    "    model.add(keras.layers.Dense(100, activation=\"selu\", kernel_initializer=\"lecun_normal\"))\n",
    "\n",
    "# Use alpha dropout in the last hidden layer.\n",
    "model.add(keras.layers.AlphaDropout(rate=0.2))\n",
    "    \n",
    "# Output layer.\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile the model\n",
    "You must at least specify the loss function and the optimizer to use. You can also specify a list of additional metrics to use during training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The optimixer is Stochastic Gradient Descent with momentum optimization. The momentum is set to 0.9.\n",
    "# This value usually works well in practice. We use the default learning rate (0.01).\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.SGD(momentum=0.9),\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EarlyStopping (with rollback to the best model).\n",
    "early_stopping = keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
    "\n",
    "# Performance scheduling\n",
    "# (multiply the learning rate by a factor when the error stops dropping for a number of epochs, called patience)\n",
    "lr_scheduler = keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=4)\n",
    "\n",
    "# Train the model with early stopping or performance scheduling or both. Training is much faster when\n",
    "# early stopping is used, but a slightly better accuracy is achieved with performance scheduling alone.\n",
    "history = model.fit(X_train_standardized, y_train, epochs=40,\n",
    "                    validation_data=(X_valid_standardized, y_valid),\n",
    "                    callbacks=[lr_scheduler, early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the learning curves.\n",
    "# (The training curves should be shifted half an epoch to the left to be completely comparable with\n",
    "# the validation curves).\n",
    "\n",
    "pd.DataFrame(history.history).plot(figsize=(8, 5))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_test_standardized, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "nav_menu": {
   "height": "264px",
   "width": "369px"
  },
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
